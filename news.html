<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Technology Co-operation</title>
    <link href="style.css" rel="stylesheet">
    <!--jquery library (code I have used)-->
    <script src="scripts/jquery.js"></script>
     <!--jquery user interface script (code i have used) -->
    <script src="scripts/jquery-ui.js"></script>
    <!--colour change dom manipulation script for all buttons on this page (script I have written)-->
    <script src="scripts/DOM-Manipulation-newspage.js"></script>
    <!--Jquery script to show and hide div tags when the show and hide article buttons are pressed (script I have written)-->
    <script src="scripts/showhide_divs_jquery.js"></script>
</head>


    
    
    
<body>

<!--header-->    
    
<header>
    <div class="container">
        <div id="logo-brand">
            <a id="logo-link" href="index.html"><img src="img/logo.png" alt="main company logo"></a>
        </div>
 <!--nav bar-->   
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="news.html">Tech on the News</a></li>
            <li><a href="deals.html">Top Deals</a></li>
            <li><a href="expo.html">Expo</a></li>
            <li><a href="contact-info.html">Contact Us</a></li>
            <li><a href="report.html">Report</a></li>
        </ul>
    </nav>
    
   </div> 
    
</header>


<section id="main-content">
 <!--first article heading-->   
<div class="container">
    <div class="article-title">
        <h1><img class="article_image" src="img/article-1.jpg" alt="image of hyundai vehicle">
             HYUNDAI SHOWS OFF 'WALKING CAR' AT CES EXPO
        </h1>
         
        <button id="open-article1" type="button" value="view article" onmouseover="changeCbtn1()" onmouseout="backtoNbtn1()">View Article</button>
        <button id="close-article1" type="button" value="hide-article" onmouseover="changeCbtn2()" onmouseout="backtoNbtn2()">Hide Article</button>
         
    </div>
</div>     
  

    
<!--first article content-->
    
<div class="container">  
    <div class="article-content" id="article-1">
        <img src="img/article-1.jpg" alt="image of hyundai vehicle">   
        <!--rticle taken from www.newscientist.com-->
        <p>Hyundai has shown off a small model of a car it says can activate robotic legs to walk at 3mph (5km/h) over rough terrain.

            Also able to climb a 5ft (1.5m) wall and jump a 5ft gap, the Hyundai Elevate could be useful for emergency rescues following natural disasters, it said.

            It was part of a project exploring "beyond the range of wheels", it added.

            The concept has been in development for three years and was unveiled at the CES technology fair in Las Vegas.

            "When a tsunami or earthquake hits, current rescue vehicles can only deliver first responders to the edge of the debris field. They have to go the rest of the way by foot," said Hyundai vice-president John Suh.</p>
    
        <p>"Elevate can drive to the scene and climb right over flood debris or crumbled concrete."

            Mr Suh also suggested that wheelchair users could be collected via the vehicles, which could "walk" up to the front door of a building with step-only access.</p>
    
        <p>Prof David Bailey, from Aston Business School, said: "Often car companies bring out lots of concepts which may or may not make it into production but it's great to think in new       ways about mobility.

            "For most of us, it's going to be wheels and roads but in extreme situations there may be scope for this sort of thing.

            "There may well be applications in terms of emergency services - but there are very big technological challenges to make this sort of thing."</p>
    </div>   
    
  
</div> 
    
   

    
  
    
    

    
<!--second article heading-->    
    
<div class="container">
    
    <div class="article-title">
      
      
        <h1> <img class="article_image" src="img/article_2.jpg" alt="man sniffing his vest" >
            COMPUTER CHIP WILL SNIFF YOUR ARMPITS AND TELL YOU WHEN YOU HAVE BO
        </h1>  
      
        <button id="open-article2" type="button" onmouseover="changeCbtn3()" onmouseout="backtoNbtn3()" >View Article</button>
        <button id="close-article2" type="button" onmouseover="changeCbtn4()" onmouseout="backtoNbtn4()" >Hide Article</button>
      
      
    </div>
</div>  
 <!--second article content--->   
<div class="container">  
    <div class="article-content" id="article-2">
        <img src="img/article_2.jpg" alt="man sniffing his vest">   
        <!--article taken from www.newscientist.com-->
        <p>DO I smell? It is an embarrassing problem we have all had to deal with. A run for the bus or a hot meeting room can leave you trying to check your armpit without anyone noticing.    Luckily, AI is here to help.

            UK chip-maker Arm, better known for developing the hardware that powers most smartphones, is working on a new generation of smart chips that embed artificial intelligence inside devices. One of these chips is being taught to smell.

            The idea is that the chips will be small and cheap enough to be built into clothing, allowing an AI to keep tabs on your BO throughout the day. Arm also wants to add the chips to food packaging to monitor freshness.</p>


        <p>The e-noses are part of a project called PlasticArmPit, in which Arm is developing smart chips made from thin sheets of plastic. Each chip will have eight different sensors and      built-in machine learning circuit.

            It will look like a piece of cling-film with bits stuck to it, says James Myers at Arm. “PlasticArmPit will be the first application of machine learning in plastic electronics.”

            Smells are made up of different combinations and concentrations of gases. The sensors on the chip will detect different chemicals in the air and the AI will take that complex data and identify it as a particular whiff.

            The chip will then score the smell. If it is in the armpit of your shirt, it will tell you the strength of your body odour from 1 to 5, says Myers. “It’s the job of the machine learning to collect and interpret all the data and then alert the user if action is needed.”</p>
    
    
        <p>E-noses are not new. Julian Gardner, who pioneered the technology at the University of Warwick, UK, has been building them for three decades. In 1993 he co-founded a company        called Alpha MOS that sells e-noses to the food industry.

            The trouble is that these devices cost around $20,000, says Gardner. He has since developed smaller, cheaper versions that cost just a few dollars. But they need to be made even cheaper to be sewn into clothing, which is what Arm hopes to do.

            “I think that if the sensors are almost free, then people could buy clothes with them,” says Gardner. But they will also need to survive in the wash, which could be a challenge even for plastic electronics, which are normally more resilient than regular electronics.</p>
    
    
        <p>Alex Bond at Fresh Check, a London-based start-up that is developing a chemical test to check for bacteria on food, thinks e-noses are a good way to monitor food quality because    they do not need to touch the food.

            An AI-powered nose could also be tuned to pick up different types of smell. “Flexibility is important because beef doesn’t spoil in the same way as fish,” Bond says. “And a pork loin may be classed as spoiled, but still be suitable to be turned into sausage.”

            However, Bond thinks that it will be a challenge to get smart chips into food packaging – no matter how cheap they are. “Any increase to packaging costs is hard to justify,” he says. “Most food manufacturers have exceptionally tight profit margins, so there has to be an incredibly strong incentive for them to adopt more expensive packaging.”

            One option may be to limit the use of sensors to premium foods or countries where there is a higher risk of contamination.

            Still, Arm hopes to embed more than just e-noses into packaging. Chips built into plastic could be used to signal what kind of plastic a bottle or wrapper was made from, for example.</p>
    

    </div>   
    
</div>    
     
<!--third article headung-->    
<div class="container">
    
    <div class="article-title">
      
      
    <h1> <img class="article_image" src="img/article-3.jpg" alt="device inside of computer generated model of human brain">
         WIRELESS 'PACEMAKER FOR THE BRAIN' COULD OFFER NEW TREATMENT FOR NEUROLOGICAL DISORDERS
    </h1>  
      
        <button id="open-article3" type="button" onmouseover="changeCbtn5()" onmouseout="backtoNbtn5()" >View Article</button>
        <button id="close-article3" type="button" onmouseover="changeCbtn6()" onmouseout="backtoNbtn6()"  >Hide Article</button>
      
      
    
    </div> 
</div>
<!--third article content-->  
<div class="container">  
    <div class="article-content" id="article-3" >
        <img src="img/article-3.jpg" alt="device inside of computer generated model of human brain">
        <!--article taken from www.newscientist.com-->
        <p>A new neurostimulator developed by engineers at the University of California, Berkeley, can listen to and stimulate electric current in the brain at the same time, potentially      delivering fine-tuned treatments to patients with diseases like epilepsy and Parkinson's.</p>
    
        <p>The device, named the WAND, works like a "pacemaker for the brain," monitoring the brain's electrical activity and delivering electrical stimulation if it detects something          amiss.

            These devices can be extremely effective at preventing debilitating tremors or seizures in patients with a variety of neurological conditions. But the electrical signatures that precede a seizure or tremor can be extremely subtle, and the frequency and strength of electrical stimulation required to prevent them is equally touchy. It can take years of small adjustments by doctors before the devices provide optimal treatment.

            WAND, which stands for wireless artifact-free neuromodulation device, is both wireless and autonomous, meaning that once it learns to recognize the signs of tremor or seizure, it can adjust the stimulation parameters on its own to prevent the unwanted movements. And because it is closed-loop -- meaning it can stimulate and record simultaneously -- it can adjust these parameters in real-time.</p>
    
        <p>"The process of finding the right therapy for a patient is extremely costly and can take years. Significant reduction in both cost and duration can potentially lead to greatly improved outcomes and accessibility," said Rikky Muller assistant professor of electrical engineering and computer sciences at Berkeley. "We want to enable the device to figure out what is the best way to stimulate for a given patient to give the best outcomes. And you can only do that by listening and recording the neural signatures."

            WAND can record electrical activity over 128 channels, or from 128 points in the brain, compared to eight channels in other closed-loop systems. To demonstrate the device, the team used WAND to recognize and delay specific arm movements in rhesus macaques. The device is described in a study that appeared today (Dec. 31) in Nature Biomedical Engineering.</p>
    
        <h2>Ripples in a pond</h2>
    
        <p>Simultaneously stimulating and recording electrical signals in the brain is much like trying to see small ripples in a pond while also splashing your feet -- the electrical          signals from the brain are overwhelmed by the large pulses of electricity delivered by the stimulation.

            Currently, deep brain stimulators either stop recording while delivering the electrical stimulation, or record at a different part of the brain from where the stimulation is applied -- essentially measuring the small ripples at a different point in the pond from the splashing.

            "In order to deliver closed-loop stimulation-based therapies, which is a big goal for people treating Parkinson's and epilepsy and a variety of neurological disorders, it is very important to both perform neural recordings and stimulation simultaneously, which currently no single commercial device does," said former UC Berkeley postdoctoral associate Samantha Santacruz, who is now an assistant professor at the University of Texas in Austin.</p>
    
        <p>Researchers at Cortera Neurotechnologies, Inc., led by Rikky Muller, designed the WAND custom integrated circuits that can record the full signal from both the subtle brain waves    and the strong electrical pulses. This chip design allows WAND to subtract the signal from the electrical pulses, resulting in a clean signal from the brain waves.

            Existing devices are tuned to record signals only from the smaller brain waves and are overwhelmed by the large stimulation pulses, making this type of signal reconstruction impossible.

            "Because we can actually stimulate and record in the same brain region, we know exactly what is happening when we are providing a therapy," Muller said.

            In collaboration with the lab of electrical engineering and computer science professor Jan Rabaey, the team built a platform device with wireless and closed-loop computational capabilities that can be programmed for use in a variety of research and clinical applications.</p>
    
        <p>In experiments lead by Santacruz while a postdoc at UC Berkeley, and by and electrical engineering and computer science professor Jose Carmena, subjects were taught to use a        joystick to move a cursor to a specific location. After a training period, the WAND device was capable of detecting the neural signatures that arose as the subjects prepared to      perform the motion, and then deliver electrical stimulation that delayed the motion.

            "While delaying reaction time is something that has been demonstrated before, this is, to our knowledge, the first time that it has been demonstrated in a closed-loop system based on a neurological recording only," Muller said.

            "In the future we aim to incorporate learning into our closed-loop platform to build intelligent devices that can figure out how to best treat you, and remove the doctor from having to constantly intervene in this process," said Muller said.</p>


    </div>   
    
</div>    
    
    
 

    
    
<!--fourth article heading-->
    
<div class="container">
    
    <div class="article-title">
      
      
         <h1> <img class="article_image" src="img/article-4.jpg" alt="drone next to an commercial aircraft">
             WHY THE GATWICK DRONES WERE SO HARD TO STOP
         </h1>  
      
        <button id="open-article4" type="button" onmouseover="changeCbtn7()" onmouseout="backtoNbtn7()" >View Article</button>
        <button id="close-article4" type="button" onmouseover="changeCbtn8()" onmouseout="backtoNbtn8()">Hide Article</button>
      
      
    </div>
<!--fourth article content-->    
<div class="container">  
    <div class="article-content" id="article-4">
        <img src="img/article-4.jpg" alt="drone next to an commercial aircraft">   
        <!--article taken from www.newscientist.com-->
        <p>Just after 21:00 on December 19 two drones were spotted near the runway of Gatwick airport. Though the drones disappeared for a short time around 03:00 on December 20, allowing      the runway to reopen temporarily, they reappeared, shutting down the airport for the entire day.


            Gatwick is a massive mover of people around the world. Last month 3.05 million passengers travelled through its terminals, an increase of nearly six per cent on the previous year. In 2017, 282,000 flights arrived and departed from the airport – around 772 “aircraft movements” every day.

            Yesterday, Gatwick was due to fly 110,000 passengers on 760 separate flights, with 10,000 additional passengers being affected by the drone disruption late on December 19. At lesat 650 of these flights were cancelled.</p>
    
        <p>The impact of stopping those movements even for a few hours, as demonstrated by the drones still hovering near Gatwick’s runway, is enormous. Flights have been diverted to           airports around the country at a time when airspace is at its busiest as people try to travel home for Christmas. The knock-on effect is also huge, with tightly-timetabled           aircraft movements left out of sync. Small disruptions can take days to correct.

            The latest from Gatwick – on the morning of December 21 – is that the airport's runway is in use again with a "limited number" of flights arriving and departing. Airport officials expect almost 700 flights to leave Gatwick today, with around 100 being cancelled. Passengers have been advised to check with airlines about their individual flights</p>
    
        <p>However, the person (or peeople) flying the drones have not yet been found. Chris Woodroofe, the airport's chief operating officer, said flights leaving as normal depended on        whether or not the drone reappeared.

            On the evening of December 20, transport secretary Chris Grayling said drones have been seen each time the airport tried to reopen a runway. Reports said there had been around 50 sightings of drones over the airport. "We have called in government agencies and the military to assist us in getting Gatwick open again to counteract this unprecedented event, this criminal act," he told the BBC.

            The incident has drawn criticism from prime minister Theresa May who said the actions of the drone flyers and said more legislation will be considered.

            The UK defence secretary Gavin Williamson also said armed forces have been drafted in to help deal with the drones. Reporters say a "unique military capability" will be used to help the situation. No further information has been given.</p>
    
        <p>A spokesperson for NATS, the organisation that oversees the UK’s airspace, says it has been working closely with airports and airlines to mitigate the disruption.

            “Flying any kind of drone near an airport or in controlled airspace without the proper permissions is dangerous and unacceptable,” the NATS spokesperson added. “People using drones should apply common sense when deciding where to fly and need to remember that the same legal obligations apply to them as well as any other pilot.”

            NATS has worked closely with industry bodies, such as the Civil Aviation Authority (CAA), which governs the airspace above the country, and the government “to create an environment that ensures the safety of all airspace users while supporting the growing use of drones,” they said.

            “There’s a constant problem and the CAA have known about this for years,” explains Andrew Heaton, an independent drone expert. “It’s about trying to make people aware there are laws and regulations in place.”</p>
    
    <p>
        Thursday briefing: Unauthorised drones suspend flights at Gatwick airport
        Gatwick Airport aerial photo
        Thursday briefing: Unauthorised drones suspend flights at Gatwick airport
        By WIRED

        While the pilots of the drones have still not been found, what they’re doing is illegal. On social media, police have requested anyone with knowledge of who may be behind the incident to come forward.

        “At the end of July this year the CAA revised the air navigation order to make it explicit that you can’t fly a drone within a kilometre of an airport, regardless of where you take off from,” explains Owen McAree, a drone expert at Liverpool John Moores University.

        There are strict rules about flying drones under 400 feet. “That 400 feet is to try and maintain separation of drones and unmanned aircraft from manned aviation,” says Heaton. The lowest-flying planes – something like a lightweight two-man Cessna – flies at around 500 feet.The revision to regulations didn’t make flying a drone near an airport illegal – “it’s always been illegal to endanger an aircraft and fly over people,” McAree says – but made it more explicit. The change in regulations was brought about after a spate of amateur drone hobbyists flying their unmanned aircraft near airports, often to get footage of planes taking off and landing. From no incidents in 2013 near airports, nearly 100 were recorded last year, according to the UK Airprox Board, with this year’s figures showing an increase.The reason there are such stringent rules around drones near airports is the potential damage they can wreak. “The easiest comparison to make is with bird strikes,” says Heaton. “When a bird hits an aircraft, it can damage the windscreen or if it goes into the engine, it can take the plane down.”
    </p>
    
        <h3>How can drones be stopped?</h3>

        <p>Many people are developing anti-drone technology, but there isn't yet a widely accepted way to deal with them. There are plenty of startups developing tech that can interrupt and    override the radio signal from the controller to the drone. Australian firm DroneShield has created a giant gun-like device that is said to jam 2.4 and 5.8 GHz frequencies and      can shoot down a drone from 2km away. DroneShield says its technology was used to bring down a drone at the February 2018 Commonwealth Games.

            An alternative way to control drones is to introduce GPS geofencing. This means a drone's software does not allow it to enter certain geographical areas that have been listed as prohibited. There's a 30-nautical mile radius around the Ronald Reagan National Airport where drones aren't able to fly.

            Others have dabbled with net cannons mounted onto other drones. In 2016, Dutch police were experimenting with training eagles to swoop and pluck rouge drones out of the air. The plan caught the eye of the UK's Met Police but it never followed-up with the plan.“There is technology out there that can help with this, but it’s not widely available or known about,” says McAree. </p>
    
        <p>“Nobody wants a situation like this,” says Rupert Dent of ARPAS UK, a drone industry users’ association. “The technology that’s available to do that right now this second is not    as good as it’s going to be. There’s quite a lot of further development to be done, but a lot of companies have been researching counter-drone technology for a while.”

            While no-one knows the motive of whoever is piloting the drones hovering around Gatwick, McAree doesn’t think it’s an amateur hobbyist in over their head. “In this case it looks more deliberate,” he says, citing the fact that standard batteries only allow drones to fly for 10 or 15 minutes at a time.But while plenty of people are left lingering in airport terminals, there is a silver lining. “I’ve had conversations in the past about the potential for people to use these for nefarious purposes,” explains McAree. “Trying to deliberately bring down an aircraft is almost a non-starter. It’s just causing huge disruption like this.”</p>
    

    </div>   
</div>    
    
    
</div> 
    
 <!--fifth article heading-->   
<div class="container">
    
    <div class="article-title">
      
      
    <h1> <img class="article_image" src="img/article-5.jpg" alt="bus advertising hydrogen energy on the back of it">
         DEVICE THAT WORKS LIKE A LUNG MAKES CLEAN FUEL FROM WATER
        
    </h1>  
      
        <button id="open-article5" type="button" onmouseover="changeCbtn9()" onmouseout="backtoNbtn9()" >View Article</button>
        <button id="close-article5" type="button"  onmouseover="changeCbtn10()" onmouseout="backtoNbtn10()" >Hide Article</button>
      
      
    </div>
    
    
</div>
    
 <!--fifth article content-->   
<div class="container">  
    <div class="article-content" id="article-5">
        <img src="img/article-5.jpg" alt="bus advertising hydrogen energy on the back of it">   
        <!--article taken from www.newscientist.com-->
        <p>Human lungs move gas through a thin membrane, extracting oxygen and sending it into our blood stream. Now a device uses the same principle to power the reactions used for making    hydrogen fuel.

            Yi Cui at Stanford University and his colleagues set out to mimic human lungs to increase the efficiency of electrocatalysts, materials that increase the rate of chemical reactions used to produce hydrogen by splitting water. Improving the process could make better fuel cells, which are used to power hydrogen vehicles and could one day be used for powering everything from cell phones to cities.</p>
    
        <p>Cui and his team made a 12-nanometer thick plastic film with tiny pores on one side which repel water. The other side is coated with gold and platinum nanoparticles that are involved in the chemical reactions. Then they rolled the film and sealed the edges to make a small pouch with the metal layer on the inside.</p>
    
        <p>When they apply a voltage to water to split it into its constituent parts, the hydrogen and oxygen gases enter the lung-like apparatus and create energy as they pass through the    conductive metals on the inside of the pouch.

            Carbon-based films that are usually used in fuel cells can create bubbles during this process, which causes energy loss. But these new lung-like devices minimise bubbles because the small pores control the rate at which gas can pass through the membrane and the pressure inside.

            Cui and his team found that their lung-like device was 32 per cent more efficient at converting energy than using the same membrane in a flat configuration. “The geometry is important,” says Cui.</p>
    
        <p>The material is stable over long periods, too. When the team ran the reaction through the lung-like architecture for 250 hours, it retained 97 per cent of its catalytic activity.    A traditional carbon-based membrane decayed to 74 per cent of its activity over just 75 hours.

            The next step is to set up a system with many of these small pouches, but it may not look like a lung. “The lung has a branching structure, but we need to have an anode and cathode for each of these, and it’s not that efficient to have a lot of those close together. The lung taught us how to deliver the gas. We’re going to have to find another form to scale this up,” says Cui.</p>
    

    </div>   
    
</div>
    
<!--sixth article heading-->
    
<div class="container">
    
    <div class="article-title">
      
      
        <h1> <img class="article_image" src="img/article-6.jpg" alt="two people sitting on the couch with internet enabled devices">
             5G WILL LET USERS DITCH FIXED-LINE HOME BROADBAND, SAYS THREE
         
        </h1>  
      
        <button id="open-article6" type="button" onmouseover="changeCbtn11()" onmouseout="backtoNbtn11()">View Article</button>
        <button id="close-article6" type="button"  onmouseover="changeCbtn12()" onmouseout="backtoNbtn12()">Hide Article</button>
      
      
    </div>
    
</div>
 <!--sixth article content-->   
<div class="container">  
    <div class="article-content" id="article-6">
        <img src="img/article-6.jpg" alt="two people sitting on the couch with internet enabled devices">   
        <!--article taken from www.newscientist.com-->
        <p>5G mobile data will be so reliable and fast most homes will no longer need a separate home broadband connection, according to one of the companies planning to launch a UK            service.

            Three UK's chief executive told BBC News there would be enough capacity on 5G to cope with demand, meaning households would be able to save money by ending their fixed-line contracts.

            He predicts consumers will use 13 times as much mobile data in 2025 as today.

            But one expert warned against "hype"</p>
    
        <p>Three has said it intends to launch its first 5G services in the UK as soon as the middle of next year.

            Its announcement coincides with news from BT's mobile division, EE, that it has switched on nine 5G trial sites in London.

            Vodafone and Telefonica-owned O2 have also bought spectrum to launch 5G services of their own in the country.</p>
    
        <h2>Higher Capacity</h2>
    
        <p>In theory, 5G could offer download speeds of up to 10 gigabits per second or even 20Gbps - although these are unlikely to be attained for many years if at all.

            Most handsets are not yet capable of pushing 4G speeds to their limits, so UK networks are under pressure to convince the public of the need to upgrade having spent more than had been predicted on the spectrum auctioned to date.As part of its pitch, Three is making the case that 5G will offer a "genuine alternative" to fixed-line copper and fibre services."Maybe not for the whole country, but certainly a significant majority of the country, I strongly believe 5G can offer a good enough home broadband experience for people to effectively ditch their copper [or fibre] connection," said David Dyson, Three UK's chief executive.</p>
    
        <p>"Maybe not for the whole country, but certainly a significant majority of the country, I strongly believe 5G can offer a good enough home broadband experience for people to           effectively ditch their copper [or fibre] connection," said David Dyson, Three UK's chief executive.

            "The challenge in terms of why we can't do that today is that the mobile networks don't have the capacity with 3G or 4G. 5G changes all of that."

            Capacity refers to the amount of data that can be handled at any one time rather than the speed.

            Three already provides a 4G-based "unlimited data" home broadband service in London, called Relish, which it acquired last year.

            But Mr Dyson said the business had to be careful how many people it signed up, to prevent its service degrading.

            This, he said, would not be a problem with 5G.

            But one industry-watcher said it was still unclear how reliable the technology would be.</p>
    
        <p>"Stability is important for video streaming at HD and Ultra HD quality levels, and paramount for the gaming community," said Andrew Ferguson, from the news site Thinkbroadband.

            "Full-fibre services are going to beat 5G as you have a connection as stable as the one that will be feeding the mobile masts and thus the variables of signal strength dropping due to a bus passing the home are avoided."

            The government is currently pursuing a target of "full-fibre" broadband coverage to the whole UK by 2033, in which high-speed optical cables are used to bring data right up to buildings without having to rely on slower copper for part of the journey.

            At present, only 5% of all properties have access to the full-fibre connections, according to the regulator Ofcom.</p>
    
        <p>But Three's chief executive suggested the cost involved could help make 5G a more attractive option.

            "Fibre-to-the-home for the small number of customers who value it and need it will probably provide a faster speed," Mr Dyson said.

            "But I think for the majority of people, 5G will be a genuine alternative.

            "It's still quite unclear to me, as I'm sure it is to many people, what is going to be the price of all these fibre-to-the-home deployments when it actually arrives

            "It's expensive to dig up roads. It takes a lot of time and money.

            "It's much cheaper and quicker to provide that connectivity via a wireless connection."</p>


    </div>   
    
</div> 
    
    

 <!--seventh article heading-->   
    
<div class="container">
    
    <div class="article-title">
      
      
    <h1> <img class="article_image" src="img/article-7.jpg" alt="Samsung Micro LED TV">
          IS THIS THE FUTURE OF TV?
         
    </h1>  
      
        <button id="open-article7" type="button" onmouseover="changeCbtn13()" onmouseout="backtoNbtn13()" >View Article</button>
        <button id="close-article7" type="button" onmouseover="changeCbtn14()" onmouseout="backtoNbtn14()" >Hide Article</button>
      
    </div>
    
    
</div>
  <!--seventh article content-->  
<div class="container">  
    <div class="article-content" id="article-7">
        <img src="img/article-7.jpg" alt="Samsung Micro LED TV">   
        <!--article taken from www.newscientist.com-->
        <p>Samsung believes that Micro LED is the future of television, and it might be right with the self-emissive technology offering a number key advantages over both QLED and OLED.        Micro LED uses a panel where each pixel is composed of microscopic red, green and blue LEDs. This approach offers the best of both OLED and LCD technologies, with the deep blacks    and wider viewing angles of the former and the increased brightness and bigger colour gamut of the latter.</p>

        <p>This approach means that Micro LED should ultimately be capable of delivering the full potential of high dynamic range (HDR), with a maximum peak brightness of 10,000 nits and      100% of the Rec.2020 wide colour gamut. The technology also delivers incredibly fast response times which should keep gamers happy, and the use of LEDs means there’s no danger of    the panel suffering from image retention or screen burn, even after hours of gaming.</p>
    
        <p>"For decades, Samsung has lead the way in next-generation display innovation", said Jonghee Han, President of Visual Display Business at Samsung Electronics. "Our Micro LED           technology is at the forefront of the next screen revolution with intelligent, customisable displays that excel in every performance category. Samsung Micro LED has no               boundaries, only endless possibilities.” 

            At last year’s CES Samsung introduced Micro LED by unveiling The Wall, a 146” Micro LED display. Due to technical advancements in the ultra-fine pitch semiconductor packaging process, the gap between the tiny LED chips has been narrowed and Samsung is now able to create a 4K Micro LED display in a smaller, more home-friendly 75” form factor. No release date has been announced yet but with Samsung able to create smaller screen sizes, actual retail units can’t be far off.</p>
    
        <p>The big difficulty with Micro LED is actually making the panels themselves. The problem is that a 4K image is composed of over eight million pixels and each pixel is comprised of    three nano inorganic LEDs – that means Samsung has to fit 24 million LEDs on to a single panel. Achieving that goal without any dead pixels requires a serious level of precision,    which makes Micro LED rather difficult to manufacture. The larger the panel size, the easier that task is and for that reason Micro LED lends itself to big screen TVs.

            Since the average TV screen size is getting bigger year-on-year that shouldn't be an issue and to demonstrate the full potential of a larger Micro LED panel, Samsung also had an 84” model on show. This TV perfectly demonstrated the full potential of the technology, with a wider colour gamut that gives images real impact. When you combine the absolute blacks of Micro LED with its brighter peak highlights, the resulting images also have incredible dynamic range and more realism.</p>
    
        <p>While much of the attention was being grabbed by the relatively smaller 75- and 84-inch TVs, Samsung was also demonstrating the latest version of The Wall. The 146" version was      unveiled at CES 2018 but this year the company was demonstrating how Micro LED can go beyond a display technology to become an integral part of the home. The panels have an          incredibly long lifespan and since here’s no danger of screen burn, you don't need to turn The Wall off when it’s in Samsung’s low power Ambient mode.

            One of the big downsides to large screen TVs is that when they aren't being used, you have a massive black rectangle on your wall. Samsung addressed this problem in 2018 with the launch of their Ambient mode, which allowed the TV to display the images, the news or the latest weather while using very little power. You can do the same with The Wall, or even use it to see and control other smart devices in your home. You can even use the Ambient mode to allow The Wall to simply vanish into your actual wall by copying it.
        </p>
    
        <p>One of the major design aspects of The Wall is that it uses a modular design, which basically means the screen is composed of a number of smaller panels. That means you can          customise your Micro LED display to fit the dimensions of your room, creating a screen of the size, resolution and aspect ratio you require. The technology also optimises the        content no matter the size and shape of the screen, scaling it to increase the resolution while keeping the pixel density constant.

            At the Samsung's First Look event the company demonstrated a 219” version of The Wall, but you can make the Micro LED screen as large as you like, choose between different aspect ratios like 16:9 or 21:9, and even support resolutions up to 8K. Finally, because Micro LED displays are bezel-free there are no borders between modules – even when you add more. The result is a seamless infinity pool effect that allows the display to elegantly blend into any living environment.

            Micro LED has enormous potential, not only as a display technology but also as a way of implementing big screen entertainment and smart features into the home in a way that is unobtrusive and completely integrated. The future looks bright... very bright.</p>


    </div>   
    
</div>   

    
    
    

   <!--eighth article heading--> 
    
<div class="container">
    
    <div class="article-title">
      
      
    <h1> <img class="article_image" src="img/article-8.jpg" alt="A single tesla vehicle at one stage on its production line">
         TESLA SAYS ITS NEW SELF-DRIVING CHIP IS FINALLY BAKED!
         
    </h1>  
      
        <button id="open-article8" type="button" onmouseover="changeCbtn15()" onmouseout="backtoNbtn15()" >View Article</button>
        <button id="close-article8" type="button"  onmouseover="changeCbtn16()" onmouseout="backtoNbtn16()" >Hide Article</button>
      
      
    </div>
    
    
</div> 
  <!--eighth article content-->  
<div class="container">  
    <div class="article-content" id="article-8">
        <img src="img/article-8.jpg" alt="A single tesla vehicle at one stage on its production line">   

        <!--article taken from www.newscientist.com-->
        <p>TESLA'S MODEL S sedan, the car company’s flagship vehicle, was first shown as a prototype in 2009, has been on sale since 2012, and, barring one small change to remove the fake grille at the front, has looked exactly the same for nearly a decade. This is notable because most manufacturers fully redesign their cars every four to six years to keep them fresh—and to keep buyers buying. For Tesla, tech upgrades are the selling point. The company pushes software updates several times a year, adding features like summon, where a car pulls in and out of a garage with nobody inside, or camper-mode, for sleeping in the car with the heating on.</p>
    
        <p>Tesla’s biggest claim is that one day, all the cars it’s currently building will be capable of full-self driving. So when Elon Musk announced after the company's second-quarter earnings report that Tesla is developing its own computer chips, it was a momentous claim by the car company. Musk's grand reveal (which maybe actually isn't news given Musk said this at an AI conference in December, a fact WIRED reported on then) led him to boast the company has “the world's most advanced computer designed specifically for autonomous operation.”</p>
    
        <p>Musk says the new silicon is an order of magnitude faster than the chips in their cars now, a product developed by NVIDIA, an industry leader and supplier to at least 20 other robocar developers. The NVIDIA chips, Musk says, can deal with 200 frames of video per second, from the cameras around the car. (Unlike most other autonomous vehicle developers, Tesla is determined that cameras are enough to perceive the world, and it doesn’t need the more expensive lidar sensors.) Tesla’s chip can handle 2,000 frames per second, with some spare capacity left for redundancy and safety.</p>
    
        <p>That might be true, but it’s not a Ruffles to Ruffles comparison. “The performance claims are against what they have in the vehicle today, which are three years old,” says NVIDIA’s director of automotive Danny Shapiro. NVIDIA’s latest silicon is at least 10 times faster than that, which would put it on a par with Tesla’s chip.</p>
    
        <p>Tesla's chip development is key to improving Autopilot. Right now the term is an umbrella for a collection of driver assistance tools, but in October 2016 Elon Musk claimed that all new Teslas now come with the hardware—cameras, ultrasonic sensors, and an on-board supercomputer—for self driving. They just needed to develop the software.

            It turns out that computer wasn’t powerful enough. Tesla upgraded it once, a year later, but now says it’ll have to upgrade all the cars again, with even more number crunching power—and the new chip. All the connectors on the computer, which sits behind the glove box in the Model S and X, are exactly the same, so it’s just a case of service centers plugging in a new one. “It's super kick-ass,” says Musk.

            LEARN MORE</p>
    
        <p>Building its own chips is the latest example of Tesla’s drive for full vertical integration, where it designs and makes everything in-house. Most automakers have chains of          suppliers and sub-suppliers, but Tesla famously, and unusually, even builds its own seats. That allows it to create custom solutions, using the ground-up redesign approach that      Musk prefers.

            For the chips, the team went back to a “bare metal level” and instead of layering up GPUs, which were originally designed to speed up graphics processing for gaming machines, Tesla designed the calculator and memory circuits from scratch, sitting right next to each other, so they can pass data at high speed, which has been a traditional choke point when a GPU is used in emulation mode to run an AI neural net.

            “The chips are up and working, and we have drop-in replacements for S, X and 3,” says Peter Bannon, who leads Tesla’s computer hardware team, and who previously designed high-spec processors for Apple’s iPhones</p>
    
        <p>Tesla’s approach might be unconventional, but that’s hardly unusual for the company. And it's a clever idea, says Jeff Miller, engineering practice professor in the computer        science department at USC. “Instead of using a generic graphics processing unit, they’re able to customize it, because they know exactly what the data coming in is, and what        they’re expecting to get back out of it.”But Shapiro says NVIDIA’s chips are heavily customized too. “With a GPU, even though the name hasn’t changed, the architecture is            fundamentally different,” he says. They now have deep learning accelerators, and new data links to remove bottlenecks and choke points, speeding them up dramatically. And, he        says, sometimes a broader focus is desirable. NVIDIA’s stacks are designed to run the other applications self-drivers will need, like mapping and path planning, not just image      processing.

            Losing Tesla as a customer isn’t going to impact NVIDIA; the company says sales were so small that it won’t have a material impact. But it does highlight the importance of computing power in the self-driving car industry. These vehicles will have to run heavy duty supercomputers, and their builders will have to find a way to power, and cool, these machines, which run hot and hungry. Making the best chip, hidden away inside, could prove more important than fancy looks or screaming performance.</p>
    
        <p>For Tesla owners, there’s still no timeline for those promised full self-driving capabilities, but Musk did say a coast-to-coast demonstration drive might be possible by the end    of the year. (Something he also said last year.)

            Tesla says some improvements to in Autopilot will come in a couple of months, with software V9.0. That could mean the introduction of what the company calls “on-ramp to off ramp” driving, where the car handles lane changes on the freeway automatically. But it’s mostly going to be about safety improvements, which might help counter some of the criticism of the feature’s safety record so far, with recent crashes highlighting its deficiencies.</p>
    
        <p>“I think we're starting to see a new set of safety features that really only make sense in this world where we have an extremely high understanding of what's happening around the vehicle,” says Stuart Bowers, Tesla’s VP of engineering. He didn’t give specifics, but a car that can see all around itself, all the time, could nudge forwards to avoid a rear-end collision for example, or refuse to open doors when a cyclist is passing, and certainly should be better at avoiding highway barriers.

            And if the features don’t live up to expectations, Tesla’s ready with a distraction. Elon Musk says the V9.0 update will bring classic Atari games to Tesla's giant screens. Computer control of a Pole Position car is a lot easier than a real one.</p>


    </div>   
    
</div>   
    

</section>
    
 <!--footer-->   
<footer>
    
    <p> Technology Haven Copyright &copy; 2019</p>
    
    
</footer>  
    
</body>
</html>









 


